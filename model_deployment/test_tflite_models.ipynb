{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da3dfdd",
   "metadata": {},
   "source": [
    "# EchoCare Demo - Cry Detection & Classification (TFLite)\n",
    "\n",
    "This notebook demonstrates the two-stage inference pipeline using **TensorFlow Lite** models:\n",
    "1. **Stage 1**: Cry Detection (cry vs non-cry)\n",
    "2. **Stage 2**: Cry Classification (hungry vs pain)\n",
    "\n",
    "**Changes from Keras version:**\n",
    "- Uses TFLite interpreter instead of Keras models\n",
    "- Preprocessing remains identical\n",
    "- Ready for Raspberry Pi deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce8d9e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import tensorflow as tf  # For TFLite interpreter\n",
    "import time  # To measure inference speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db739d7",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set parameters to match the training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a6aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model paths\n",
    "cry_detection_tflite_path = Path('tflite_models/detection_model.tflite')\n",
    "cry_classification_tflite_path = Path('tflite_models/classification_model.tflite')\n",
    "\n",
    "# Audio processing parameters (matches training configuration)\n",
    "sample_rate = 16000\n",
    "duration = 1.0\n",
    "n_mels = 128  # mel bands for spectrogram\n",
    "target_size = (224, 224)  # MobileNetV2 input size\n",
    "\n",
    "# Normalization parameters from training (Z-score standardisation)\n",
    "cry_detection_mean = -37.628456115722656\n",
    "cry_detection_std = 22.107717514038086\n",
    "cry_classification_mean = -40.55323028564453\n",
    "cry_classification_std = 19.64647102355957\n",
    "\n",
    "# Confidence thresholds\n",
    "cry_detection_threshold = 0.85  # 85% confidence for cry detection\n",
    "cry_classification_threshold = 0.70  # 70% confidence for cry classification\n",
    "\n",
    "# Class labels\n",
    "cry_types = {\n",
    "    0: \"Pain\",\n",
    "    1: \"Hungry\"\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "print(f\"  Sample Rate: {sample_rate}Hz\")\n",
    "print(f\"  Duration: {duration}s\")\n",
    "print(f\"  Mel Bands: {n_mels}\")\n",
    "print(f\"  Target Size: {target_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fc2707",
   "metadata": {},
   "source": [
    "## 3. Audio Preprocessing Functions\n",
    "\n",
    "### Preprocessing Pipeline:\n",
    "1. Load and Resample (to 16kHz)\n",
    "2. Ensure 1 sec duration (pad/trim)\n",
    "3. Create mel-spectrogram (128, 32)\n",
    "4. Convert to dB scale\n",
    "5. Resize to MobileNetV2's input size (224 x 224)\n",
    "6. Convert grayscale to RGB (1 channel â†’ 3 channels)\n",
    "7. Normalise mel-spectrogram (to [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01648e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(audio_path, mean, std):\n",
    "    \"\"\"\n",
    "    Load audio file and prepare for model input following the exact training pipeline.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to audio file\n",
    "        mean: Training mean for Z-score normalization\n",
    "        std: Training std for Z-score normalization\n",
    "    \"\"\"\n",
    "    print(f\"\\nLoading audio: {audio_path}\")\n",
    "    \n",
    "    # Step 1: Load audio file at target sample rate\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate, duration=duration, mono=True)\n",
    "    \n",
    "    # Pad or trim to exact duration\n",
    "    target_length = int(sample_rate * duration)\n",
    "    if len(audio) < target_length:\n",
    "        audio = np.pad(audio, (0, target_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:target_length]\n",
    "    \n",
    "    print(f\"Audio loaded: {len(audio)} samples at {sample_rate}Hz\")\n",
    "    \n",
    "    # Step 2: Convert to mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sample_rate,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    print(f\"Mel-spectrogram created: {mel_spec.shape}\")\n",
    "    \n",
    "    # Step 3: Convert to log scale (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    print(f\"Converted to dB scale\")\n",
    "    \n",
    "    # Step 4: Z-score standardization using training statistics\n",
    "    mel_spec_normalized = (mel_spec_db - mean) / std\n",
    "    print(f\"Standardized using mean={mean:.2f}, std={std:.2f}\")\n",
    "    \n",
    "    # Step 5: Prepare for MobileNetV2 (resize + RGB)\n",
    "    prepared_spec = prepare_for_mobilenet(mel_spec_normalized)\n",
    "    \n",
    "    print(f\"Final shape for model input: {prepared_spec.shape}\")\n",
    "    \n",
    "    return prepared_spec\n",
    "\n",
    "\n",
    "def prepare_for_mobilenet(spectrogram, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Prepare mel-spectrogram for MobileNetV2 input.\n",
    "    \n",
    "    Args:\n",
    "        spectrogram: numpy array mel-spectrogram (already standardized)\n",
    "        target_size: tuple (height, width) for resizing\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Add channel dimension if needed\n",
    "    if len(spectrogram.shape) == 2:\n",
    "        spectrogram = np.expand_dims(spectrogram, axis=-1)\n",
    "    \n",
    "    # Step 2: Resize to MobileNetV2's expected input size\n",
    "    resized = cv2.resize(spectrogram, target_size)\n",
    "    \n",
    "    # Step 3: Ensure channel dimension exists after resize\n",
    "    if len(resized.shape) == 2:\n",
    "        resized = np.expand_dims(resized, axis=-1)\n",
    "    \n",
    "    # Step 4: Convert grayscale (1 channel) to RGB (3 channels)\n",
    "    if resized.shape[-1] == 1:\n",
    "        resized = np.repeat(resized, 3, axis=-1)\n",
    "    \n",
    "    # Step 5: Add batch dimension\n",
    "    resized = np.expand_dims(resized, axis=0)\n",
    "    \n",
    "    return resized.astype(np.float32)\n",
    "\n",
    "print(\"Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a71082",
   "metadata": {},
   "source": [
    "## 4. TFLite Inference Functions\n",
    "\n",
    "**These functions replace Keras `model.predict()` with TFLite interpreter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9813a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tflite_inference(interpreter, input_data):\n",
    "    \"\"\"\n",
    "    Run inference using TFLite interpreter.\n",
    "    \n",
    "    Args:\n",
    "        interpreter: TFLite interpreter instance\n",
    "        input_data: Preprocessed input data (numpy array)\n",
    "        \n",
    "    Returns:\n",
    "        prediction: Model output (numpy array)\n",
    "    \"\"\"\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Ensure input data type matches model expectation\n",
    "    input_data = input_data.astype(input_details[0]['dtype'])\n",
    "    \n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Get output tensor\n",
    "    prediction = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def stage_1_cry_detection(interpreter, audio_data):\n",
    "    \"\"\"\n",
    "    Stage 1: Detect if audio contains a cry using TFLite model.\n",
    "    \n",
    "    Args:\n",
    "        interpreter: TFLite interpreter for detection model\n",
    "        audio_data: Preprocessed mel-spectrogram\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_cry, confidence_score)\n",
    "    \"\"\"\n",
    "    print(\"\\nStage 1: Cry Detection (TFLite)\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run inference using TFLite\n",
    "    prediction = run_tflite_inference(interpreter, audio_data)\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "    \n",
    "    # Get confidence score (probability of cry)\n",
    "    confidence = float(prediction[0][0])\n",
    "    \n",
    "    # Determine if cry detected based on threshold\n",
    "    is_cry = confidence >= cry_detection_threshold\n",
    "    \n",
    "    print(f\"Inference Time: {inference_time:.2f} ms\")\n",
    "    print(f\"Confidence Score: {confidence:.1%}\")\n",
    "    print(f\"Detection Threshold: {cry_detection_threshold:.1%}\")\n",
    "    print(f\"Result: {'CRY DETECTED' if is_cry else 'NO CRY DETECTED'}\")\n",
    "    \n",
    "    return is_cry, confidence\n",
    "\n",
    "\n",
    "def stage_2_cry_classification(interpreter, audio_data):\n",
    "    \"\"\"\n",
    "    Stage 2: Classify cry type (hungry vs pain) using TFLite model.\n",
    "    \n",
    "    Args:\n",
    "        interpreter: TFLite interpreter for classification model\n",
    "        audio_data: Preprocessed mel-spectrogram\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cry_type, confidence_score)\n",
    "    \"\"\"\n",
    "    print(\"\\nStage 2: Cry Classification (TFLite)\")\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Run inference using TFLite\n",
    "    prediction = run_tflite_inference(interpreter, audio_data)\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "    \n",
    "    # Get raw prediction value\n",
    "    raw_value = float(prediction[0][0])\n",
    "    \n",
    "    print(f\"Inference Time: {inference_time:.2f} ms\")\n",
    "    print(f\"Raw Prediction Value: {raw_value:.4f}\")\n",
    "    \n",
    "    # Determine predicted class based on threshold 0.5\n",
    "    if raw_value >= 0.5:\n",
    "        predicted_class = 1  # Hungry\n",
    "        confidence = raw_value  # How far above 0.5\n",
    "    else:\n",
    "        predicted_class = 0  # Pain\n",
    "        confidence = 1 - raw_value  # How close to 0\n",
    "    \n",
    "    cry_type = cry_types[predicted_class]\n",
    "    \n",
    "    print(f\"Threshold: 0.50\")\n",
    "    print(f\"Predicted Class: {cry_type}\")\n",
    "    print(f\"Confidence Score: {confidence:.1%}\")\n",
    "    print(f\"Classification Threshold: {cry_classification_threshold:.1%}\")\n",
    "    \n",
    "    # Check if confidence meets threshold\n",
    "    if confidence >= cry_classification_threshold:\n",
    "        print(f\"Result: CLASSIFICATION CONFIDENT\")\n",
    "        return cry_type, confidence\n",
    "    else:\n",
    "        print(f\"Result: CLASSIFICATION UNCERTAIN (below threshold)\")\n",
    "        return None, confidence\n",
    "\n",
    "\n",
    "print(\"TFLite inference functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afae66d",
   "metadata": {},
   "source": [
    "## 5. Load TFLite Models\n",
    "\n",
    "Load the trained cry detection and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d72144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading TFLite models:\\n\")\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Checking cry detection TFLite model...\")\n",
    "print(f\"  Path: {cry_detection_tflite_path}\")\n",
    "print(f\"  Exists: {cry_detection_tflite_path.exists()}\")\n",
    "print(f\"  Is file: {cry_detection_tflite_path.is_file()}\")\n",
    "\n",
    "print(f\"\\nChecking cry classification TFLite model...\")\n",
    "print(f\"  Path: {cry_classification_tflite_path}\")\n",
    "print(f\"  Exists: {cry_classification_tflite_path.exists()}\")\n",
    "print(f\"  Is file: {cry_classification_tflite_path.is_file()}\")\n",
    "\n",
    "try:\n",
    "    # Load detection model\n",
    "    cry_detection_interpreter = tf.lite.Interpreter(\n",
    "        model_path=str(cry_detection_tflite_path)\n",
    "    )\n",
    "    cry_detection_interpreter.allocate_tensors()\n",
    "    print(f\"\\nCry detection TFLite model loaded\")\n",
    "    \n",
    "    # Print input/output details\n",
    "    det_input = cry_detection_interpreter.get_input_details()[0]\n",
    "    det_output = cry_detection_interpreter.get_output_details()[0]\n",
    "    print(f\"   Input shape: {det_input['shape']}\")\n",
    "    print(f\"   Output shape: {det_output['shape']}\")\n",
    "    \n",
    "    # Load classification model\n",
    "    cry_classification_interpreter = tf.lite.Interpreter(\n",
    "        model_path=str(cry_classification_tflite_path)\n",
    "    )\n",
    "    cry_classification_interpreter.allocate_tensors()\n",
    "    print(f\"\\nCry classification TFLite model loaded\")\n",
    "    \n",
    "    # Print input/output details\n",
    "    cls_input = cry_classification_interpreter.get_input_details()[0]\n",
    "    cls_output = cry_classification_interpreter.get_output_details()[0]\n",
    "    print(f\"   Input shape: {cls_input['shape']}\")\n",
    "    print(f\"   Output shape: {cls_output['shape']}\")\n",
    "    \n",
    "    print(\"\\nAll TFLite models loaded successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR loading TFLite models: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798e2c3",
   "metadata": {},
   "source": [
    "## 6. Run Inference on Test Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test audio file path\n",
    "hungry_sample_1 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/cry/hungry_0046017001.wav\")\n",
    "hungry_sample_2 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/cry/hungry_0083007001.wav\")\n",
    "normal_sample = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/cry/normal_0052039000.wav\")\n",
    "\n",
    "pain_sample_1 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/cry/pain_0013011002.wav\")\n",
    "pain_sample_2 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/cry/pain_0022003002.wav\")\n",
    "\n",
    "non_cry_sample_1 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/non-cry/vacuum_cleaner_5-263902-A-36_seg01.wav\")\n",
    "non_cry_sample_2 = Path(\"C:/Users/danel/FYP/echocare-infant-cry-classification/dataset/processed/cry_detection/test/non-cry/coughing_5-204604-A-24_seg00.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1967a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_path = hungry_sample_1 # choose which sample to test\n",
    "\n",
    "\n",
    "print(\"ECHOCARE DEMO - CRY DETECTION & CLASSIFICATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc77859",
   "metadata": {},
   "source": [
    "### 6.1 Preprocess Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4de136",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Preprocess for detection using detection normalisation parameters\n",
    "    audio_data_detection = load_and_preprocess_audio(\n",
    "        test_audio_path, \n",
    "        cry_detection_mean, \n",
    "        cry_detection_std\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR preprocessing audio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa42542",
   "metadata": {},
   "source": [
    "### 6.2 Stage 1: Cry Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    is_cry, detection_confidence = stage_1_cry_detection(\n",
    "        cry_detection_interpreter,  # Using TFLite interpreter\n",
    "        audio_data_detection\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR in cry detection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9cddcb",
   "metadata": {},
   "source": [
    "### 6.3 Stage 2: Cry Classification (only if cry detected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa2f3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess for classification using classification normalisation parameters\n",
    "if is_cry:\n",
    "    try:\n",
    "        audio_data_classification = load_and_preprocess_audio(\n",
    "            test_audio_path,\n",
    "            cry_classification_mean,\n",
    "            cry_classification_std\n",
    "        )\n",
    "        \n",
    "        cry_type, classification_confidence = stage_2_cry_classification(\n",
    "            cry_classification_interpreter,  # Using TFLite interpreter\n",
    "            audio_data_classification  # Use classification-specific preprocessing\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR in cry classification: {e}\")\n",
    "        cry_type = None\n",
    "        classification_confidence = 0.0\n",
    "else:\n",
    "    print(\"\\nSkipping Stage 2 (no cry detected)\")\n",
    "    cry_type = None\n",
    "    classification_confidence = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3601c5",
   "metadata": {},
   "source": [
    "### 6.4 Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d2ef09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"FINAL RESULT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if not is_cry:\n",
    "    print(f\"No cry detected (confidence: {detection_confidence:.1%})\")\n",
    "elif cry_type is not None:\n",
    "    # Confident classification\n",
    "    print(f\"Baby is crying: {cry_type}\")\n",
    "    print(f\"  Classification confidence: {classification_confidence:.1%}\")\n",
    "else:\n",
    "    # Cry detected but classification uncertain\n",
    "    print(f\"Baby is crying ({detection_confidence:.1%})\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
